# -*- coding: utf-8 -*-
import asyncio
from openai import AsyncOpenAI
import time
from typing import List, Dict
import json
from datetime import datetime
from httpx import AsyncHTTPClient, Timeout
import platform
import signal

# =======================================================================
#                         å…¨å±€é…ç½®åŒºåŸŸ (éœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)
# =======================================================================
class Config:
    # ç±»OpenAIæœåŠ¡é…ç½®ï¼ˆåƒå¸†ã€DeepSeekç­‰ï¼‰
    OPENAI_CLASS_APIS = [
        {
            "name": "Kimiæ™ºèƒ½åŠ©æ‰‹",
            "base_url": "https://api.moonshot.cn/v1",
            "api_key": "sk-iVZ3N2WBXyUGJLOLbJXgVbZbRcIwrNAXPKlUpZOF4IKrdzSB",
            "model": "moonshot-v1-128k",
            "timeout": 45
        },
        {
            "name": "DeepSeeké€šç”¨ç‰ˆ",
            "base_url": "https://api.chatanywhere.tech/v1",
            "api_key": "sk-lCsmD3pXeLBV7MbA9IkhY0vRbYEb6BJQlEiFU3JaHyoheeOB",
            "model": "deepseek-chat",
            "timeout": 30
        }
    ]
    
    # Ollamaå¤šæœåŠ¡å™¨é›†ç¾¤é…ç½®
    OLLAMA_CLUSTER = [
        {"model": "deepseek-r1:8b", "base_url": "http://121.43.148.249:11434"},
        {"model": "deepseek-r1:32b", "base_url": "http://123.114.208.42:11434"},
        {"model": "qwen:0.5b", "base_url": "http://222.95.140.221:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://122.96.100.79:11434"},
        {"model": "deepseek-r1:70b", "base_url": "http://183.61.5.17:11434"},
        {"model": "deepseek-r1:1.5b", "base_url": "http://125.122.34.216:11434"},
        {"model": "Qwen2.5-VL:7b", "base_url": "http://218.255.138.150:11434"},
        {"model": "llama3-taide-lx-8b-chat-alpha1", "base_url": "http://218.255.138.150:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://118.201.252.28:11434"},
        {"model": "qwen2.5:72b", "base_url": "http://91.219.165.104:11434"},
        {"model": "gemma3:27b", "base_url": "http://142.132.157.27:11434"},
        {"model": "deepseek-r1:14b", "base_url": "http://31.6.100.43:11434"},
        {"model": "gemma3:12b", "base_url": "http://218.255.138.150:11434"},
    ]
    
    # æ€»ç»“ç”ŸæˆæœåŠ¡é…ç½®
    SUMMARY_SERVICE = {
        "base_url": "https://api.moonshot.cn/v1",
        "api_key": "sk-iVZ3N2WBXyUGJLOLbJXgVbZbRcIwrNAXPKlUpZOF4IKrdzSB",
        "model": "moonshot-v1-128k",
        "timeout": 60,
        "prompt_template": """ä½œä¸ºèµ„æ·±åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼š
1. æå–3ä¸ªæ ¸å¿ƒå…±è¯†å’Œ2ä¸ªä¸»è¦åˆ†æ­§ç‚¹
2. å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è§£é‡Šæ·±åº¦
3. ç”¨Markdownæ ¼å¼è¾“å‡ºæŠ¥å‘Šï¼ŒåŒ…å«ï¼š
   - æ ¸å¿ƒç»“è®ºï¼ˆæ ‡æ³¨æ”¯æŒæ¨¡å‹æ•°é‡ï¼‰
   - å…±è¯†åˆ†æ
   - äº‰è®®ç„¦ç‚¹
   - å¯é æ€§è¯„ä¼°ï¼ˆåŸºäºå“åº”æˆåŠŸç‡ï¼‰"""
    }

# =======================================================================
#                           æ ¸å¿ƒå¼•æ“å®ç°
# =======================================================================
class AIAnalyst:
    def __init__(self):
        self.clients = {}  # å®¢æˆ·ç«¯è¿æ¥æ± 
        self.results = []  # å­˜å‚¨æ‰€æœ‰ç»“æœ
        self._setup_interrupt_handler()
        
        # æ€§èƒ½ç›‘æ§æŒ‡æ ‡
        self.stats = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "latencies": [],
            "start_time": time.time()
        }

    def _setup_interrupt_handler(self):
        """æ³¨å†Œä¿¡å·å¤„ç†å™¨å®ç°ä¼˜é›…é€€å‡º"""
        if platform.system() != 'Windows':
            signal.signal(signal.SIGINT, self._handle_interrupt)

    def _handle_interrupt(self, signum, frame):
        """å¤„ç†CTRL+Cä¿¡å·"""
        print("&#92;nğŸ›‘ æ£€æµ‹åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜æ—¥å¿—...")
        self._save_operation_logs()
        exit(0)

    async def _get_client(self, config: Dict) -> AsyncOpenAI:
        """è·å–æˆ–åˆ›å»ºå¼‚æ­¥å®¢æˆ·ç«¯å®ä¾‹"""
        client_key = f"{config['base_url']}|{config.get('api_key', 'no_key')}"
        
        if client_key not in self.clients:
            self.clients[client_key] = AsyncOpenAI(
                base_url=config["base_url"],
                api_key=config.get("api_key"),
                timeout=config.get("timeout", 30),
                http_client=AsyncHTTPClient(
                    timeout=Timeout(config.get("timeout", 30)),
                    limits=httpx.Limits(max_connections=100)
                )
            )
        return self.clients[client_key]

    async def _execute_api_request(self, config: Dict, prompt: str) -> Dict:
        """æ‰§è¡Œå•ä¸ªAPIè¯·æ±‚"""
        start_time = time.perf_counter()
        result_template = {
            "config": config,
            "success": False,
            "content": "",
            "error": "",
            "latency_ms": 0
        }
        
        try:
            client = await self._get_client(config)
            
            # å¯¹OllamaæœåŠ¡è¿›è¡Œæ¨¡å‹å¯ç”¨æ€§æ£€æŸ¥
            if "api_key" not in config:
                available_models = await client.models.list()
                if config["model"] not in [m.id for m in available_models.data]:
                    raise ValueError(f"æ¨¡å‹ {config['model']} æœªæ‰¾åˆ°")
            
            # æ‰§è¡Œå®é™…è¯·æ±‚
            response = await client.chat.completions.create(
                model=config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1000
            )
            
            result_template.update({
                "success": True,
                "content": response.choices[0].message.content
            })
            
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            result_template["error"] = error_msg
            self.stats["failed_requests"] += 1
        finally:
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            latency = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            result_template["latency_ms"] = latency
            self.stats["total_requests"] += 1
            if result_template["success"]:
                self.stats["successful_requests"] += 1
                self.stats["latencies"].append(latency)
                
            return result_template

    def _save_operation_logs(self):
        """ä¿å­˜æ“ä½œæ—¥å¿—åˆ°æ–‡ä»¶"""
        log_data = {
            "timestamp": datetime.now().isoformat(),
            "performance_metrics": self.stats,
            "request_details": [{
                "config": r["config"],
                "success": r["success"],
                "latency": r["latency_ms"],
                "error": r.get("error", "")
            } for r in self.results]
        }
        with open("operation_logs.json", "w", encoding="utf-8") as f:
            json.dump(log_data, f, indent=2, ensure_ascii=False)

    async def _generate_summary_report(self, prompt: str) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        try:
            summary_client = await self._get_client(Config.SUMMARY_SERVICE)
            response = await summary_client.chat.completions.create(
                model=Config.SUMMARY_SERVICE["model"],
                messages=[
                    {"role": "system", "content": Config.SUMMARY_SERVICE["prompt_template"]},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=1500
            )
            return response.choices[0].message.content
        except Exception as e:
            return f"æ€»ç»“ç”Ÿæˆå¤±è´¥: {str(e)}"

# =======================================================================
#                           ç”¨æˆ·ç•Œé¢æ¨¡å—
# =======================================================================
class ResultVisualizer:
    @staticmethod
    def display_realtime_progress(analyst: AIAnalyst):
        """å®æ—¶æ˜¾ç¤ºè°ƒç”¨è¿›åº¦"""
        elapsed = time.time() - analyst.stats["start_time"]
        avg_latency = sum(analyst.stats["latencies"])/len(analyst.stats["latencies"]) if analyst.stats["latencies"] else 0
        print(f"&#92;rğŸ“ˆ å®æ—¶çŠ¶æ€ | æˆåŠŸ: {analyst.stats['successful_requests']} å¤±è´¥: {analyst.stats['failed_requests']} | "
              f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms | è¿è¡Œæ—¶é—´: {elapsed:.1f}s", end="")

    @staticmethod
    def display_final_results(results: List[Dict]):
        """æ˜¾ç¤ºæœ€ç»ˆç»“æœ"""
        print("&#92;n" + "="*100)
        print("ğŸ“‹ è¯¦ç»†å“åº”ç»“æœ:")
        for idx, res in enumerate(results, 1):
            status = "âœ…" if res["success"] else "âŒ"
            print(f"&#92;n{idx:02d}. [{status}] {res['config'].get('name', res['config']['model'])}")
            print(f"   ğŸ“Œ ç«¯ç‚¹: {res['config']['base_url']}")
            print(f"   â±ï¸ å»¶è¿Ÿ: {res['latency_ms']:.1f}ms")
            
            if res["success"]:
                print(f"   ğŸ“ å†…å®¹: {res['content'][:250]}...")
            else:
                print(f"   ğŸ”´ é”™è¯¯: {res['error']}")
            print("-"*100)

# =======================================================================
#                           ä¸»ç¨‹åºå…¥å£
# =======================================================================
async def main():
    analyst = AIAnalyst()
    configs = Config.OPENAI_CLASS_APIS + Config.OLLAMA_CLUSTER
    
    try:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_prompt = input("ğŸ“ è¯·è¾“å…¥æ‚¨è¦åˆ†æçš„é—®é¢˜ï¼š")
    except KeyboardInterrupt:
        print("&#92;nğŸ›‘ ç”¨æˆ·å–æ¶ˆè¾“å…¥")
        return

    # å¯åŠ¨å¼‚æ­¥ç›‘æ§ä»»åŠ¡
    monitor_task = asyncio.create_task(_monitor_progress(analyst))
    
    # æ‰§è¡Œæ‰€æœ‰APIè°ƒç”¨
    print("&#92;nğŸš€ å¼€å§‹å¹¶å‘è°ƒç”¨APIé›†ç¾¤...")
    tasks = [analyst._execute_api_request(cfg, user_prompt) for cfg in configs]
    results = await asyncio.gather(*tasks)
    analyst.results = results
    
    # æ¸…ç†ç›‘æ§ä»»åŠ¡
    monitor_task.cancel()
    try:
        await monitor_task
    except asyncio.CancelledError:
        pass

    # æ˜¾ç¤ºç»“æœ
    ResultVisualizer.display_final_results(results)
    analyst._save_operation_logs()

    # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
    if any(res["success"] for res in results):
        print("&#92;nğŸ§  æ­£åœ¨ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š...")
        summary = await analyst._generate_summary_report(user_prompt)
        print("&#92;n" + "="*100)
        print(summary)
        print("="*100 + "&#92;n")
    else:
        print("&#92;nâš ï¸ æ‰€æœ‰APIè°ƒç”¨å¤±è´¥ï¼Œæ— æ³•ç”Ÿæˆæ€»ç»“æŠ¥å‘Š")

async def _monitor_progress(analyst: AIAnalyst):
    """å®æ—¶ç›‘æ§ä»»åŠ¡"""
    while True:
        ResultVisualizer.display_realtime_progress(analyst)
        await asyncio.sleep(0.5)

if __name__ == "__main__":
    asyncio.run(main())
