# -*- coding: utf-8 -*-
import asyncio
from openai import AsyncOpenAI
import time
from typing import List, Dict
import json
from datetime import datetime
from httpx import AsyncClient as AsyncHTTPClient, Timeout
import platform
import signal

# ===================== å…¨å±€é…ç½® ===================== #
class Config:
    # OpenAIç±»æœåŠ¡é…ç½®
    OPENAI_CLASS_APIS = [
        {
            "name": "Kimi",
            "base_url": "https://api.moonshot.cn/v1",
            "api_key": "sk-iVZ3N2WBXyUGJLOLbJXgVbZbRcIwrNAXPKlUpZOF4IKrdzSB",
            "model": "moonshot-v1-128k",
            "timeout": 45
        }
    ]
    
    # Ollamaé›†ç¾¤é…ç½®ï¼ˆä¿æŒåŸæ ·ï¼‰
    OLLAMA_CLUSTER = [
        {"model": "deepseek-r1:8b", "base_url": "http://121.43.148.249:11434"},
        {"model": "deepseek-r1:32b", "base_url": "http://123.114.208.42:11434"},
        {"model": "qwen:0.5b", "base_url": "http://222.95.140.221:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://122.96.100.79:11434"},
        {"model": "deepseek-r1:70b", "base_url": "http://183.61.5.17:11434"},
        {"model": "deepseek-r1:1.5b", "base_url": "http://125.122.34.216:11434"},
        {"model": "Qwen2.5-VL:7b", "base_url": "http://218.255.138.150:11434"},
        {"model": "cwchang/llama3-taide-lx-8b-chat-alpha1:latest", "base_url": "http://218.255.138.150:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://118.201.252.28:11434"},
        {"model": "qwen2.5:72b", "base_url": "http://91.219.165.104:11434"},
        {"model": "gemma3:27b", "base_url": "http://142.132.157.27:11434"},
        {"model": "deepseek-r1:14b", "base_url": "http://31.6.100.43:11434"},
        {"model": "gemma3:12b", "base_url": "http://218.255.138.150:11434"},
    ]
    
    # ä¿®æ”¹åçš„æ€»ç»“æœåŠ¡é…ç½®
    SUMMARY_SERVICE = {
        "base_url": "https://api.openai.com/v1",  # OpenAIå®˜æ–¹æ¥å£
        "api_key": "sk-your-openai-key-here",     # æ›¿æ¢ä¸ºæœ‰æ•ˆOpenAIå¯†é’¥
        "model": "gpt-4o-mini",                   # æŒ‡å®šæ–°æ¨¡å‹
        "timeout": 60,
        "prompt_template": """ä½œä¸ºèµ„æ·±åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹å›ç­”ï¼š
1. æç‚¼3ä¸ªæ ¸å¿ƒå…±è¯† 
2. è¯†åˆ«2ä¸ªä¸»è¦åˆ†æ­§ç‚¹
3. æŒ‰ã€ä¿¡æ¯å®Œæ•´æ€§ã€‘å’Œã€é€»è¾‘ä¸¥è°¨æ€§ã€‘è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰
4. ç”¨Markdownè¾“å‡ºæŠ¥å‘Šï¼ŒåŒ…å«ï¼š
   - æ€»ä½“ç»“è®º
   - è¯¦ç»†åˆ†æ
   - æ¨¡å‹èƒ½åŠ›è¯„ä¼°"""
    }

# ===================== æ ¸å¿ƒå¼•æ“ ===================== #
class AIAnalyst:
    def __init__(self):
        self.clients = {}
        self.results = []
        self._setup_interrupt_handler()
        
        # ç›‘æ§æ•°æ®
        self.stats = {
            "total": 0,
            "success": 0,
            "latencies": [],
            "start_time": time.time()
        }

    def _setup_interrupt_handler(self):
        """å¤„ç†CTRL+Cä¿¡å·"""
        if platform.system() != 'Windows':
            signal.signal(signal.SIGINT, self._handle_interrupt)

    def _handle_interrupt(self, signum, frame):
        """ä¼˜é›…é€€å‡ºå¤„ç†"""
        print("&#92;nğŸ›‘ ä¸­æ–­æ£€æµ‹ï¼Œä¿å­˜æ—¥å¿—ä¸­...")
        self.save_logs()
        exit(0)

    async def get_client(self, config: Dict) -> AsyncOpenAI:
        """è·å–å¸¦è¿æ¥æ± çš„å®¢æˆ·ç«¯ï¼ˆä¿®å¤ç‰ˆï¼‰"""
        key = f"{config['base_url']}_{config.get('api_key','NO_KEY')}"
        if key not in self.clients:
            client_params = {
                "base_url": config["base_url"],
                "timeout": config.get("timeout", 30),
                "http_client": AsyncHTTPClient(timeout=Timeout(30))
            }
            
            # å¤„ç†ä¸åŒæœåŠ¡ç±»å‹çš„è®¤è¯
            if "api_key" in config:
                client_params["api_key"] = config["api_key"]
            else:
                # OllamaæœåŠ¡ç‰¹æ®Šå¤„ç†
                client_params["api_key"] = "no-key-required"  # ç»•è¿‡æ£€æŸ¥
                client_params["default_headers"] = {"Authorization": ""}  # æ¸…ç©ºè®¤è¯å¤´
            
            self.clients[key] = AsyncOpenAI(**client_params)
        return self.clients[key]

    async def call_api(self, config: Dict, prompt: str) -> Dict:
        """æ‰§è¡ŒAPIè°ƒç”¨ï¼ˆå‚æ•°ä¿®å¤ç‰ˆï¼‰"""
        start = time.perf_counter()
        result = {
            "config": config.copy(),
            "success": False,
            "content": "",
            "error": "",
            "latency": 0
        }
        
        try:
            client = await self.get_client(config)
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            request_args = {
                "messages": [{"role": "user", "content": prompt}],
                "temperature": 0.7,
                "max_tokens": 1000
            }
            
            # å¤„ç†ä¸åŒæœåŠ¡ç±»å‹
            if "api_key" in config:  # OpenAIç±»æœåŠ¡
                request_args["model"] = config["model"]
            else:  # OllamaæœåŠ¡
                request_args["model"] = config["model"].split(":")[0]  # å»é™¤ç‰ˆæœ¬åç¼€
                request_args["temperature"] = 0.5
            
            response = await client.chat.completions.create(**request_args)
            
            result.update({
                "success": True,
                "content": response.choices[0].message.content
            })
            
        except Exception as e:
            error_msg = f"{type(e).__name__}: {str(e)}"
            result["error"] = error_msg
            
        finally:
            latency = (time.perf_counter() - start) * 1000  # æ¯«ç§’
            result["latency"] = latency
            self._update_stats(result["success"], latency)
            return result

    def _update_stats(self, success: bool, latency: float):
        """æ›´æ–°ç»Ÿè®¡æ•°æ®"""
        self.stats["total"] += 1
        if success:
            self.stats["success"] += 1
            self.stats["latencies"].append(latency)

    def print_live_stats(self):
        """å®æ—¶æ˜¾ç¤ºç»Ÿè®¡æ•°æ®"""
        elapsed = time.time() - self.stats["start_time"]
        avg_latency = sum(self.stats["latencies"])/len(self.stats["latencies"]) if self.stats["latencies"] else 0
        print(f"&#92;rğŸ“Š å®Œæˆ: {self.stats['success']}/{self.stats['total']} | "
              f"æˆåŠŸç‡: {self.stats['success']/self.stats['total']*100:.1f}% | "
              f"å¹³å‡å»¶è¿Ÿ: {avg_latency:.0f}ms | "
              f"è¿è¡Œæ—¶é—´: {elapsed:.1f}s", end="")

    def save_logs(self):
        """ä¿å­˜è¯¦ç»†æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "stats": self.stats,
            "results": [{
                "config": r["config"],
                "success": r["success"],
                "latency": r["latency"]
            } for r in self.results]
        }
        with open("api_logs.json", "w") as f:
            json.dump(log_entry, f, indent=2)

# ===================== ç»“æœå¤„ç†å™¨ ===================== #
class ResultPrinter:
    @staticmethod
    def print_details(results: List[Dict]):
        """æ‰“å°è¯¦ç»†ç»“æœ"""
        print("&#92;n" + "="*80)
        print("ğŸ“„ è¯¦ç»†å“åº”ç»“æœ:")
        for idx, res in enumerate(results, 1):
            status = "âœ…" if res["success"] else "âŒ"
            print(f"&#92;n{idx:02d}. [{status}] {res['config']['model']} @ {res['config']['base_url']}")
            print(f"   â±ï¸ å»¶è¿Ÿ: {res['latency']:.0f}ms")
            if res["success"]:
                print(f"   ğŸ“ å†…å®¹: {res['content'][:200]}...")
            else:
                print(f"   ğŸ”´ é”™è¯¯: {res['error']}")
        print("="*80 + "&#92;n")

# ===================== ä¸»ç¨‹åº ===================== #
async def main():
    analyst = AIAnalyst()
    configs = Config.OPENAI_CLASS_APIS + Config.OLLAMA_CLUSTER
    
    # è·å–ç”¨æˆ·è¾“å…¥
    try:
        user_question = input("ğŸ“ è¯·è¾“å…¥åˆ†æé—®é¢˜ï¼š")
    except KeyboardInterrupt:
        print("&#92;nğŸ›‘ è¾“å…¥å–æ¶ˆ")
        return

    # å¯åŠ¨è°ƒç”¨ä»»åŠ¡
    print("&#92;nğŸš€ å¯åŠ¨APIé›†ç¾¤è°ƒç”¨...")
    tasks = [analyst.call_api(cfg, user_question) for cfg in configs]
    
    # å®æ—¶ç›‘æ§
    monitor_task = asyncio.create_task(_monitor_progress(analyst))
    
    # æ”¶é›†ç»“æœ
    results = await asyncio.gather(*tasks)
    analyst.results = results
    monitor_task.cancel()
    
    # è¾“å‡ºç»“æœ
    ResultPrinter.print_details(results)
    analyst.save_logs()

async def _monitor_progress(analyst: AIAnalyst):
    """å®æ—¶ç›‘æ§è¿›åº¦"""
    while True:
        analyst.print_live_stats()
        await asyncio.sleep(0.5)

if __name__ == "__main__":
    asyncio.run(main())
