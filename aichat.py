# -*- coding: utf-8 -*-
import asyncio
from openai import AsyncOpenAI
import time
from typing import List, Dict, Optional
import json
from datetime import datetime
import platform
import signal

# ===================== å…¨å±€é…ç½® ===================== #
class Config:
    # ç±»OpenAIæœåŠ¡é…ç½®
    OPENAI_CLASS_APIS = [
        {
            "name": "Kimi",
            "base_url": "https://api.moonshot.cn/v1",
            "api_key": "sk-iVZ3N2WBXyUGJLOLbJXgVbZbRcIwrNAXPKlUpZOF4IKrdzSB",
            "model": "moonshot-v1-128k",
            "timeout": 20
        }
    ]
    
    # Ollamaé›†ç¾¤é…ç½®
    OLLAMA_CLUSTER = [
        {"model": "deepseek-r1:8b", "base_url": "http://121.43.148.249:11434"},
        {"model": "deepseek-r1:32b", "base_url": "http://123.114.208.42:11434"},
        {"model": "qwen:0.5b", "base_url": "http://222.95.140.221:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://122.96.100.79:11434"},
        {"model": "deepseek-r1:70b", "base_url": "http://183.61.5.17:11434"},
        {"model": "deepseek-r1:1.5b", "base_url": "http://125.122.34.216:11434"},
        {"model": "Qwen2.5-VL:7b", "base_url": "http://218.255.138.150:11434"},
        {"model": "cwchang/llama3-taide-lx-8b-chat-alpha1:latest", "base_url": "http://218.255.138.150:11434"},
        {"model": "deepseek-r1:7b", "base_url": "http://118.201.252.28:11434"},
        {"model": "qwen2.5:72b", "base_url": "http://91.219.165.104:11434"},
        {"model": "gemma3:27b", "base_url": "http://142.132.157.27:11434"},
        {"model": "deepseek-r1:14b", "base_url": "http://31.6.100.43:11434"},
        {"model": "gemma3:12b", "base_url": "http://218.255.138.150:11434"},
    ]
    
    # æ€»ç»“æœåŠ¡é…ç½®
    SUMMARY_SERVICE = {
        "base_url": "https://api.chatanywhere.tech/v1",
        "api_key": "sk-lCsmD3pXeLBV7MbA9IkhY0vRbYEb6BJQlEiFU3JaHyoheeOB",
        "model": "deepseek",
        "timeout": 45,
        "prompt": """ä½œä¸ºèµ„æ·±åˆ†æå¸ˆï¼Œè¯·åŸºäºä»¥ä¸‹å†…å®¹ï¼š
1. æå–3ä¸ªå…³é”®å…±è¯†å’Œ2ä¸ªæ ¸å¿ƒäº‰è®®
2. å¯¹æ¯”ä¸åŒæ¨¡å‹çš„è§£é‡Šæ·±åº¦
3. ç”¨Markdownè¾“å‡ºï¼ŒåŒ…å«ï¼š
   - æ ¸å¿ƒç»“è®ºï¼ˆå¸¦æ¨¡å‹å¼•ç”¨ï¼‰
   - å…±è¯†åˆ†æï¼ˆæ”¯æŒæ¨¡å‹æ•°ï¼‰
   - äº‰è®®ç„¦ç‚¹
   - å¯é æ€§è¯„ä¼°ï¼ˆåŸºäºå“åº”æˆåŠŸç‡ï¼‰"""
    }

# ===================== å¼•æ“æ ¸å¿ƒ ===================== #
class AIAnalysisEngine:
    def __init__(self):
        self.client_cache = {}
        self.monitor = {
            "total": 0,
            "success": 0,
            "latency": [],
            "errors": []
        }
        self._setup_interrupt()

    def _setup_interrupt(self):
        """å¤„ç†CTRL+Cä¿¡å·"""
        if platform.system() == 'Windows':
            return
        signal.signal(signal.SIGINT, self._graceful_exit)

    def _graceful_exit(self, signum, frame):
        """ä¼˜é›…é€€å‡ºå¤„ç†"""
        print("&#92;nğŸ›‘ ä¸­æ–­æ£€æµ‹ï¼Œæ­£åœ¨ä¿å­˜æ—¥å¿—...")
        self.save_progress()
        exit(0)

    async def get_client(self, config: Dict) -> AsyncOpenAI:
        """å¸¦è¿æ¥æ± çš„å®¢æˆ·ç«¯è·å–"""
        cache_key = f"{config['base_url']}-{config.get('api_key','')}"
        if cache_key not in self.client_cache:
            self.client_cache[cache_key] = AsyncOpenAI(
                base_url=config["base_url"],
                api_key=config.get("api_key"),
                timeout=config.get("timeout", 30)
            )
        return self.client_cache[cache_key]

    async def _call_api(self, config: Dict, prompt: str) -> Dict:
        """æ‰§è¡ŒAPIè°ƒç”¨"""
        start_time = time.perf_counter()
        response = {"config": config, "success": False}
        
        try:
            client = await self.get_client(config)
            
            # æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥ï¼ˆä»…Ollamaï¼‰
            if "api_key" not in config:
                models = await client.models.list()
                if config["model"] not in [m.id for m in models.data]:
                    raise ValueError(f"Model {config['model']} not found")
            
            # æ‰§è¡Œè¯·æ±‚
            chat_completion = await client.chat.completions.create(
                model=config["model"],
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=1000
            )
            
            response.update({
                "content": chat_completion.choices[0].message.content,
                "success": True
            })
            
        except Exception as e:
            response["error"] = str(e)
            self.monitor["errors"].append({
                "config": config,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            })
            
        # æ›´æ–°ç›‘æ§æ•°æ®
        latency = (time.perf_counter() - start_time) * 1000  # æ¯«ç§’
        self.monitor["total"] += 1
        if response["success"]:
            self.monitor["success"] += 1
            self.monitor["latency"].append(latency)
        
        return response

    async def concurrent_call(self, prompt: str) -> List[Dict]:
        """æ‰§è¡Œå¹¶å‘è¯·æ±‚"""
        all_configs = Config.OPENAI_CLASS_APIS + Config.OLLAMA_CLUSTER
        tasks = [self._call_api(cfg, prompt) for cfg in all_configs]
        return await asyncio.gather(*tasks)

    def generate_report(self, results: List[Dict]) -> str:
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        success_rate = self.monitor["success"] / self.monitor["total"] * 100
        avg_latency = sum(self.monitor["latency"]) / len(self.monitor["latency"]) if self.monitor["latency"] else 0
        
        report = [
            "ğŸ“Š æ€§èƒ½æŠ¥å‘Š",
            f"- æ€»è°ƒç”¨æ•°: {self.monitor['total']}",
            f"- æˆåŠŸç‡: {success_rate:.1f}%",
            f"- å¹³å‡å»¶è¿Ÿ: {avg_latency:.1f}ms",
            f"- é”™è¯¯ç±»å‹: {len(set(e['error'] for e in self.monitor['errors']))}ç§"
        ]
        return "&#92;n".join(report)

    def save_progress(self):
        """ä¿å­˜è¿›åº¦åˆ°æ–‡ä»¶"""
        with open("progress.log", "w") as f:
            json.dump({
                "timestamp": datetime.now().isoformat(),
                "monitor": self.monitor
            }, f, indent=2)

# ===================== ç”¨æˆ·ç•Œé¢ ===================== #
class LiveDashboard:
    def __init__(self, total_tasks: int):
        self.total = total_tasks
        self.completed = 0
        self.start_time = time.time()
        
    async def update(self):
        """å®æ—¶æ›´æ–°è¿›åº¦"""
        while self.completed < self.total:
            elapsed = time.time() - self.start_time
            speed = self.completed / elapsed if elapsed > 0 else 0
            print(f"&#92;rğŸš€ è¿›åº¦: {self.completed}/{self.total} | "
                  f"é€Ÿåº¦: {speed:.1f}req/s | "
                  f"è¿è¡Œæ—¶é—´: {elapsed:.1f}s", end="")
            await asyncio.sleep(0.2)
            
    def increment(self):
        self.completed += 1

# ===================== ä¸»æµç¨‹ ===================== #
async def main():
    # åˆå§‹åŒ–å¼•æ“
    engine = AIAnalysisEngine()
    dashboard = LiveDashboard(
        total_tasks=len(Config.OPENAI_CLASS_APIS) + len(Config.OLLAMA_CLUSTER)
    )
    
    # ç”¨æˆ·äº¤äº’
    user_question = input("ğŸ“ è¯·è¾“å…¥åˆ†æé—®é¢˜ï¼š")
    
    # å¯åŠ¨ç›‘æ§
    asyncio.create_task(dashboard.update())
    
    # æ‰§è¡Œå¹¶å‘è°ƒç”¨
    print("&#92;nğŸ” æ­£åœ¨å¹¶å‘è°ƒç”¨APIé›†ç¾¤...")
    results = await engine.concurrent_call(user_question)
    
    # ç”ŸæˆæŠ¥å‘Š
    print("&#92;n&#92;n" + "="*60)
    print(engine.generate_report(results))
    
    # ä¿å­˜æ—¥å¿—
    engine.save_progress()
    
    # æ€»ç»“ç”Ÿæˆï¼ˆç¤ºä¾‹ï¼‰
    print("&#92;nğŸ§  æ­£åœ¨ç”Ÿæˆæ™ºèƒ½æ€»ç»“...")
    # æ­¤å¤„æ·»åŠ æ€»ç»“APIè°ƒç”¨é€»è¾‘

if __name__ == "__main__":
    asyncio.run(main())
