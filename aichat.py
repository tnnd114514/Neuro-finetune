# -*- coding: utf-8 -*-
import os
import asyncio
from openai import AsyncOpenAI
import json
import time
from typing import List, Dict

# ===================== ç”¨æˆ·è‡ªå®šä¹‰é…ç½®åŒºåŸŸ ===================== #
# ç±»OpenAI APIé…ç½®ï¼ˆéœ€è‡ªè¡Œç”³è¯·ï¼‰
OPENAI_CLASS_APIS = [
    {   # åƒå¸†APIç¤ºä¾‹
        "name": "qwen-plus",
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "api_key": "sk-2378af2a79be4903aa7dd861765086c7" 
        "model": "qwen-plus-latest"  # æ˜ç¡®æŒ‡å®šæ¨¡å‹ # ç™¾åº¦æ™ºèƒ½äº‘è·å–
    },
    {   # DeepSeekç¤ºä¾‹
        "name": "deepseek",
        "base_url": "https://api.chatanywhere.tech/v1,
        "api_key": "sk-lCsmD3pXeLBV7MbA9IkhY0vRbYEb6BJQlEiFU3JaHyoheeOB"
        "model": "deepseek"
    }
]

# Ollamaæœ¬åœ°æ¨¡å‹é…ç½®ï¼ˆæ— éœ€å¯†é’¥ï¼‰
OLLAMA_MODELS = [
    {"name": "deepseek-r1:8b", "base_url": "http://121.43.148.249:11434"},
    {"name": "deepseek-r1:32b", "base_url": "http://123.114.208.42:11434"},
    {"name": "qwen:0.5b", "base_url": "http://222.95.140.221:11434"},
    {"name": "deepseek-r1:7b", "base_url": "http://122.96.100.79:11434"},
    {"name": "deepseek-r1:70b", "base_url": "http://183.61.5.17:11434"},
    {"name": "deepseek-r1:1.5b", "base_url": "http://125.122.34.216:11434"},
    {"name": "Qwen2.5-VL:7b", "base_url": "http://218.255.138.150:11434"},
    {"name": "cwchang/llama3-taide-lx-8b-chat-alpha1:latest", "base_url": "http://218.255.138.150:11434"},
    {"name": "deepseek-r1:7b", "base_url": "http://118.201.252.28:11434"},
    {"name": "qwen2.5:72b", "base_url": "http://91.219.165.104:11434"},
    {"name": "gemma3:27b", "base_url": "http://142.132.157.27:11434"},
    # æ·»åŠ å…¶ä»–11ä¸ªæœ¬åœ°æ¨¡å‹...
]

# æ€»ç»“ç”¨OpenAI APIé…ç½®
SUMMARY_API = {
    "base_url": "https://api.moonshot.cn/v1",  # æˆ–æ›¿æ¢ä¸ºå…¶ä»–å…¼å®¹API
    "api_key": "sk-iVZ3N2WBXyUGJLOLbJXgVbZbRcIwrNAXPKlUpZOF4IKrdzSB",
    "model": "moonshot-v1-128k"
    "timeout": 60  # å¢å¤§æ€»ç»“è¶…æ—¶æ—¶é—´
}

# ============================================================ #

async def call_openai_api(client_config: Dict, prompt: str) -> Dict:
    """è°ƒç”¨ç±»OpenAI APIæœåŠ¡ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    try:
        client = AsyncOpenAI(
            base_url=client_config["base_url"],
            api_key=client_config["api_key"],
            timeout=client_config.get("timeout", 30),
            http_client=AsyncHTTPClient(proxies=PROXY_SETTINGS)
        )
        response = await client.chat.completions.create(
            model=client_config["model"],  # ä½¿ç”¨æ˜ç¡®æŒ‡å®šçš„æ¨¡å‹
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return {
            "model": client_config["name"],
            "content": response.choices.message.content,
            "success": True
        }
    except Exception as e:
        return {"model": client_config["name"], "content": str(e), "success": False}

async def call_ollama_api(model_config: Dict, prompt: str) -> Dict:
    """è°ƒç”¨æœ¬åœ°/è¿œç¨‹Ollama APIï¼ˆç¨³å®šæ€§å¢å¼ºç‰ˆï¼‰"""
    try:
        client = AsyncOpenAI(
            base_url=model_config["base_url"],
            timeout=60,
            http_client=AsyncHTTPClient(proxies=PROXY_SETTINGS)
        )
        response = await client.chat.completions.create(
            model=model_config["name"],  # ç›´æ¥ä½¿ç”¨å®Œæ•´æ¨¡å‹å
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7
        )
        return {
            "model": model_config["name"],
            "content": response.choices.message.content,
            "success": True
        }
    except Exception as e:
        return {"model": model_config["name"], "content": str(e), "success": False}

def format_responses(responses: List[Dict]) -> str:
    """æ ¼å¼åŒ–è¾“å‡ºï¼ˆæ·»åŠ é”™è¯¯æ ‡è¯†ï¼‰"""
    output = []
    for resp in responses:
        status = "âœ…" if resp["success"] else "âŒ"
        output.append(f"[{resp['model']}] {status}:")
        output.append(resp["content"] + "&#92;n")
    return "&#92;n".join(output)

async def main():
    user_question = input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼š")
    
    # åˆ›å»ºå¼‚æ­¥ä»»åŠ¡ï¼ˆæ·»åŠ 10%éšæœºå»¶è¿Ÿé˜²æ­¢å°ç¦ï¼‰
    tasks = []
    for config in OPENAI_CLASS_APIS:
        tasks.append(call_openai_api(config, user_question))
        await asyncio.sleep(0.1)  # æ·»åŠ è°ƒç”¨é—´éš”
    
    for config in OLLAMA_MODELS:
        tasks.append(call_ollama_api(config, user_question))
        await asyncio.sleep(0.1)
    
    # æ‰§è¡Œè°ƒç”¨å¹¶è®¡æ—¶
    start_time = time.time()
    results = await asyncio.gather(*tasks)
    print(f"&#92;nâœ… å®Œæˆ{len(results)}ä¸ªAPIè°ƒç”¨ï¼Œè€—æ—¶{time.time()-start_time:.2f}ç§’")
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    aggregated = format_responses(results)
    
    # ç”Ÿæˆæœ€ç»ˆæ€»ç»“ï¼ˆæ·»åŠ é‡è¯•æœºåˆ¶ï¼‰
    max_retries = 3
    final_summary = ""
    for attempt in range(max_retries):
        try:
            client = AsyncOpenAI(
                base_url=SUMMARY_API["base_url"],
                api_key=SUMMARY_API["api_key"],
                timeout=SUMMARY_API["timeout"],
                http_client=AsyncHTTPClient(proxies=PROXY_SETTINGS)
            )
            summary = await client.chat.completions.create(
                model=SUMMARY_API["model"],
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹åˆ†æå¸ˆ"},
                    {"role": "user", "content": f"é—®é¢˜ï¼š{user_question}&#92;n&#92;næ¨¡å‹è¾“å‡ºï¼š&#92;n{aggregated}&#92;nè¯·æ€»ç»“ä¸»è¦è§‚ç‚¹ï¼ˆ500å­—å†…ï¼‰ï¼š"}
                ],
                max_tokens=600,
                temperature=0.3
            )
            final_summary = summary.choices.message.content
            break
        except Exception as e:
            if attempt == max_retries - 1:
                final_summary = f"æ€»ç»“å¤±è´¥ï¼š{str(e)}"
            await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
    
    # æ ¼å¼åŒ–è¾“å‡º
    print("&#92;n" + "="*60)
    print(f"ğŸ“ æœ€ç»ˆæ€»ç»“ï¼ˆ{len(final_summary)}å­—ï¼‰")
    print("="*60)
    print(final_summary)
    print("="*60)

if __name__ == "__main__":
    asyncio.run(main())